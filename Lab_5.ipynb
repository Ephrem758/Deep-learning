{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMPZvh1ApaM8OtI7SnlSsn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ephrem758/Deep-learning/blob/main/Lab_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "z_GXr9Hj1J3Q"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom Dense layer\n",
        "class MyDenseLayer:\n",
        "\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        # Initialize weights and bias randomly\n",
        "        self.weights = torch.rand((input_dim, output_dim))\n",
        "        self.bias = torch.rand(output_dim)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Perform forward pass through the layer\n",
        "        self.output = torch.matmul(inputs, self.weights) + self.bias"
      ],
      "metadata": {
        "id": "l7Bh8Yst1zO4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom Sigmoid activation function\n",
        "class MySigmoidActivation:\n",
        "    def forward(self, input):\n",
        "        # Apply sigmoid activation function\n",
        "        self.output = 1 / (1 + torch.exp(-input))\n",
        "        return self.output"
      ],
      "metadata": {
        "id": "zyDA-xh713Uu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom ReLU activation function\n",
        "class MyReLUActivation:\n",
        "    def forward(self, inputs):\n",
        "        # Apply ReLU activation function\n",
        "        self.output = torch.max(inputs, torch.tensor(0))\n",
        "        return self.output"
      ],
      "metadata": {
        "id": "-UJYupEJ16C0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom Categorical Cross-Entropy Loss function\n",
        "class MyCategoricalLoss:\n",
        "    def forward(self, y_pred, y_true):\n",
        "        # Calculate categorical cross-entropy loss\n",
        "        if y_pred.shape !=  y_true.shape:\n",
        "            y_true -= 1\n",
        "            one_hot_notation = torch.zeros(y_pred.shape)\n",
        "            one_hot_notation[range(len(y_pred)), y_true] = 1\n",
        "        else:\n",
        "            one_hot_notation = y_pred\n",
        "        loss = -torch.sum(one_hot_notation * torch.log(y_pred)) / len(y_true)\n",
        "        self.output = loss\n",
        "        return loss"
      ],
      "metadata": {
        "id": "R-vPcumi18VK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom Softmax activation function\n",
        "class MySoftmaxActivation:\n",
        "    def forward(self, inputs):\n",
        "        # Apply softmax activation function\n",
        "        pow = torch.exp(inputs - torch.max(inputs, axis=1, keepdims=True)[0])\n",
        "        summ = torch.sum(pow, axis=1, keepdims=True)\n",
        "        ans = pow / summ\n",
        "        self.output = ans\n",
        "        return ans"
      ],
      "metadata": {
        "id": "f_lNO6aW1_H4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom Classification Model\n",
        "class MyClassificationModel:\n",
        "    def __init__(self):\n",
        "        # Initialize layers and activations\n",
        "        self.layer1 = MyDenseLayer(2, 2)\n",
        "        self.layer1_activation = MyReLUActivation()\n",
        "        self.output_layer = MyDenseLayer(2, 2)\n",
        "        self.output_layer_activation = MySigmoidActivation()\n",
        "        self.error = float('inf')\n",
        "\n",
        "    def forward_propagation(self, inputs):\n",
        "        # Perform forward propagation\n",
        "        self.input = inputs\n",
        "        self.layer1.forward(inputs)\n",
        "        self.layer1_activation.forward(self.layer1.output)\n",
        "        self.output_layer.forward(self.layer1_activation.output)\n",
        "        self.output_layer_activation.forward(self.output_layer.output)\n",
        "\n",
        "    def calculate_error(self, y_true):\n",
        "        # Calculate error using Mean Squared Error\n",
        "        if self.output_layer_activation.output.shape != y_true.shape:\n",
        "            one_hot_notation = torch.zeros(self.output_layer_activation.output.shape)\n",
        "            one_hot_notation[y_true] = 1\n",
        "        else:\n",
        "            one_hot_notation = y_true\n",
        "        self.y_true = one_hot_notation\n",
        "        self.error = (self.output_layer_activation.output - one_hot_notation)\n",
        "        self.mse = torch.mean(self.error)\n",
        "\n",
        "    def backward_propagation(self, learning_rate):\n",
        "        # Perform backward propagation\n",
        "        dloss_by_dsig = self.error\n",
        "        dsig_by_layer2 = (torch.tensor([1]) - self.output_layer_activation.output) * self.output_layer_activation.output\n",
        "\n",
        "        # Layer 2 backpropagation\n",
        "        back2 = dloss_by_dsig * dsig_by_layer2\n",
        "        layer2_grad = torch.cat((self.layer1_activation.output.unsqueeze(dim=0),self.layer1_activation.output.unsqueeze(dim=0)),dim=0).T * back2\n",
        "        self.output_layer.weights -= torch.tensor([learning_rate]) * layer2_grad\n",
        "        self.output_layer.bias -= torch.tensor([learning_rate]) * back2\n",
        "\n",
        "        # Layer 1 backpropagation\n",
        "        drelu_by_dlayer1 = self.layer1.output > 0\n",
        "        back1 = drelu_by_dlayer1 * torch.sum(self.output_layer.weights * back2, dim=1, keepdims=True).squeeze()\n",
        "        layer1_grad = torch.cat((self.input.unsqueeze(dim=0),self.input.unsqueeze(dim=0)),dim=0).T * back1\n",
        "        self.layer1.weights -= torch.tensor([learning_rate]) * layer1_grad\n",
        "        self.layer1.bias -= torch.tensor([learning_rate]) * back1\n",
        "\n",
        "    def train(self, inputs, y_true, epochs=1000, learning_rate=0.01):\n",
        "        # Train the model\n",
        "        for epoch in range(epochs):\n",
        "            for i in range(len(inputs)):\n",
        "                self.forward_propagation(inputs[i])\n",
        "                self.calculate_error(y_true[i])\n",
        "                if self.mse < 0.1:\n",
        "                    break\n",
        "                self.backward_propagation(learning_rate)\n",
        "        error = torch.tensor([0, 0], dtype=torch.float)\n",
        "        for i in range(len(inputs)):\n",
        "            self.forward_propagation(inputs[i])\n",
        "            self.calculate_error(y_true[i])\n",
        "            print(\"Output:\", self.output_layer_activation.output, \"Expected:\", self.y_true)\n",
        "            print(\"Squared Error:\", self.error**2)\n",
        "            print()\n",
        "            error += self.error**2\n",
        "\n",
        "        avg_error = error / len(inputs)\n",
        "        print(\"Average Squared Error:\", avg_error)\n",
        "        print(\"Overall Mean Squared Error:\", torch.mean(avg_error))"
      ],
      "metadata": {
        "id": "rnALY9cG2FBj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model and train it\n",
        "model = MyClassificationModel()\n",
        "inputs = torch.tensor([[0.5, 1.5], [1.5, 2.5], [4.5, 5.5], [6.5, 8.5], [6.5, 7.5], [0.5, 3.5]], dtype=torch.float)\n",
        "y_true = torch.tensor([1, 1, 0, 0, 0, 1])\n",
        "model.train(inputs, y_true, epochs=20000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPcWOdjO2FuF",
        "outputId": "fcc04247-795c-4a22-f8b6-ec05c85efe61"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: tensor([0.3114, 0.8883]) Expected: tensor([0., 1.])\n",
            "Squared Error: tensor([0.0969, 0.0125])\n",
            "\n",
            "Output: tensor([0.4187, 0.9692]) Expected: tensor([0., 1.])\n",
            "Squared Error: tensor([0.1753, 0.0009])\n",
            "\n",
            "Output: tensor([0.7445, 0.9995]) Expected: tensor([1., 0.])\n",
            "Squared Error: tensor([0.0653, 0.9990])\n",
            "\n",
            "Output: tensor([0.8759, 1.0000]) Expected: tensor([1., 0.])\n",
            "Squared Error: tensor([0.0154, 1.0000])\n",
            "\n",
            "Output: tensor([0.8809, 1.0000]) Expected: tensor([1., 0.])\n",
            "Squared Error: tensor([0.0142, 0.9999])\n",
            "\n",
            "Output: tensor([0.2915, 0.9674]) Expected: tensor([0., 1.])\n",
            "Squared Error: tensor([0.0850, 0.0011])\n",
            "\n",
            "Average Squared Error: tensor([0.0754, 0.5022])\n",
            "Overall Mean Squared Error: tensor(0.2888)\n"
          ]
        }
      ]
    }
  ]
}