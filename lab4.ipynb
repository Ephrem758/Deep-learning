{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVW5uKd6csI-",
        "outputId": "a6d8f8de-e318-4d3f-a46d-04f2916db3b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iterations: 115\n",
            "true vaues: tensor([[0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [1., 0., 0., 0.]])\n",
            "accuracy: tensor(0.)\n",
            "final output: tensor([[0.6733, 0.5895, 0.4999, 0.2269],\n",
            "        [0.6275, 0.6018, 0.4251, 0.1883],\n",
            "        [0.6046, 0.6079, 0.3877, 0.1690]])\n",
            "final error: tensor(0.0999)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "class DenseLayer:\n",
        "\n",
        "  # Layer initialization\n",
        "  def __init__(self, n_inputs, n_neurons):\n",
        "    # Initialize weights and biases\n",
        "    self.weights = 0.01 * torch.rand(n_inputs, n_neurons)\n",
        "    self.bias = torch.rand(n_neurons)\n",
        "\n",
        "  # Forward pass\n",
        "  def forward(self, inputs):\n",
        "    # Calculate output values from inputs, weights and biases\n",
        "    self.output = torch.matmul(inputs, self.weights) + self.bias\n",
        "\n",
        "\n",
        "class Loss_CategoricalCrossentropy:\n",
        "    def forward(self, y_pred, y_true):\n",
        "        # Ensure probabilities do not equal 0\n",
        "        y_pred_clipped = torch.clamp(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        # If targets are one-hot encoded, convert them to discrete labels\n",
        "        if len(y_true.shape) == 2:\n",
        "            y_true = torch.argmax(y_true, dim=1)\n",
        "\n",
        "        # Get the predicted probabilities for the correct classes\n",
        "        correct_confidences = y_pred_clipped[torch.arange(len(y_pred)), y_true]\n",
        "\n",
        "        # Calculate the negative log likelihood\n",
        "        negative_log_likelihoods = -torch.log(correct_confidences)\n",
        "\n",
        "        # Calculate the overall loss as the average of individual sample losses\n",
        "        loss = torch.mean(negative_log_likelihoods)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "\n",
        "class Accuracy:\n",
        "    def __init__(self):\n",
        "        self.output = 0\n",
        "    def forward(self,y_pred,y_true):\n",
        "        if y_pred.shape != y_true.shape:\n",
        "            one_hot_notation = torch.zeros(y_pred.shape)\n",
        "            one_hot_notation[range(len(y_pred)),y_true] = 1\n",
        "        else:\n",
        "            one_hot_notation = y_true\n",
        "        correct_values = y_pred==one_hot_notation\n",
        "        correct_values = correct_values * one_hot_notation\n",
        "        self.output = torch.sum(correct_values) / len(y_pred)\n",
        "        return self.output\n",
        "\n",
        "\n",
        "# Sigmoid activation\n",
        "class Activation_Sigmoid:\n",
        "  # Forward pass\n",
        "  def forward(self, inputs):\n",
        "    self.output = 1 / (1 + torch.exp(-inputs))\n",
        "\n",
        "\n",
        "\n",
        "# ReLU activation\n",
        "class Activation_ReLU:\n",
        "  # Forward pass\n",
        "  def forward(self, inputs):\n",
        "    self.output = torch.max(inputs, torch.tensor(0))\n",
        "    return self.output\n",
        "\n",
        "\n",
        "# Classification Model\n",
        "class ClassificationModel:\n",
        "    def __init__(self,num_of_features:int,num_of_classes:int):\n",
        "        self.layer1 = DenseLayer(num_of_features,4)\n",
        "        self.activation1 = Activation_ReLU()\n",
        "        self.output_layer = DenseLayer(4,num_of_classes)\n",
        "        self.output_activation = Activation_Sigmoid()\n",
        "        self.accuracy = Accuracy()\n",
        "        self.errors = [float('inf')] * num_of_classes\n",
        "\n",
        "    def forward_propagate(self,inputs):\n",
        "        self.layer1.forward(inputs)\n",
        "        self.activation1.forward(self.layer1.output)\n",
        "        self.output_layer.forward(self.activation1.output)\n",
        "        self.output_activation.forward(self.output_layer.output)\n",
        "        self.inputs = inputs\n",
        "\n",
        "    def loss_and_accuracy(self, target):\n",
        "        self.true_value = target\n",
        "        if self.output_activation.output.shape != target.shape:\n",
        "            one_hot_notation = torch.zeros(self.output_activation.output.shape)\n",
        "            one_hot_notation[range(len(self.output_activation.output)), target] = 1\n",
        "            self.true_value = one_hot_notation\n",
        "\n",
        "        squared_errors = (self.true_value - self.output_layer.output) ** 2\n",
        "        mean_loss = torch.mean(squared_errors) / 2\n",
        "\n",
        "        accuracy = self.accuracy.forward(self.output_layer.output, self.true_value)\n",
        "\n",
        "        return mean_loss, accuracy\n",
        "\n",
        "\n",
        "\n",
        "    def back_prop(self, lr):\n",
        "        errors = -(self.true_value - self.output_layer.output)  # d(y-output)/d(output)\n",
        "        sigmoid_derivative = self.output_activation.output * (1 - self.output_activation.output)\n",
        "        output_errors = errors * sigmoid_derivative\n",
        "\n",
        "        # Update weights and biases for the output layer\n",
        "        self.output_layer.bias -= lr * torch.mean(output_errors, dim=0)\n",
        "        self.output_layer.weights -= lr * torch.matmul(self.activation1.output.T, output_errors) / len(output_errors)\n",
        "\n",
        "        # Backpropagate errors to the first layer\n",
        "        relu_derivative = torch.where(self.activation1.output > 0, torch.tensor(1.0), torch.tensor(0.0))\n",
        "        hidden_errors = torch.matmul(output_errors, self.output_layer.weights.T) * relu_derivative\n",
        "\n",
        "        # Update weights and biases for the first layer\n",
        "        self.layer1.bias -= lr * torch.mean(hidden_errors, dim=0)\n",
        "        self.layer1.weights -= lr * torch.matmul(self.inputs.T, hidden_errors) / len(hidden_errors)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = ClassificationModel(2,4)\n",
        "y = torch.tensor([1,1,0])\n",
        "x = torch.tensor([[1,2],[3,4],[4,5]],dtype=torch.float)\n",
        "model.forward_propagate(x)\n",
        "model.loss_and_accuracy(y)\n",
        "\n",
        "loss = 0.1\n",
        "error = float('inf')\n",
        "iterations = 0\n",
        "while loss < error:\n",
        "    iterations += 1\n",
        "    model.forward_propagate(x)\n",
        "    model.back_prop(0.01)\n",
        "    error,acc = model.loss_and_accuracy(y)\n",
        "\n",
        "print(\"iterations:\",iterations)\n",
        "print(\"true vaues:\",model.true_value)\n",
        "print(\"accuracy:\",acc)\n",
        "print(\"final output:\",model.output_layer.output)\n",
        "print(\"final error:\",error)\n"
      ]
    }
  ]
}